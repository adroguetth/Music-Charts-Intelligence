name: Download YouTube Chart

on:
  schedule:
    - cron: '0 12 * * 1'  # Every Monday at 12:00 UTC
  workflow_dispatch:       # Manual execution
  push:
    branches:
      - main
    paths:
      - 'scripts/*.py'

env:
  RETENTION_DAYS: 30

jobs:
  download-and-store:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    permissions:
      contents: write
    
    steps:
    # 1. Checkout repository
    - name: ðŸ“š Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    # 2. Set up Python
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
        cache: 'pip'
    
    # 3. Install dependencies
    - name: ðŸ“¦ Install dependencies
      run: |
        pip install -r requirements.txt
        python -m playwright install chromium
        python -m playwright install-deps
    
    # 4. Create directory structure
    - name: ðŸ“ Create directory structure
      run: |
        mkdir -p charts_archive/databases charts_archive/backups
    
    # 5. Run download script
    - name: ðŸš€ Run download script
      run: |
        python scripts/1_descargar.py
      env:
        GITHUB_ACTIONS: true
        RETENTION_DAYS: ${{ env.RETENTION_DAYS }}
    
    # 6. Verify results
    - name: âœ… Verify results
      run: |
        echo "ðŸ“Š Verifying results..."
        
        echo "ðŸ“ Content of charts_archive/:"
        ls -lah charts_archive/
        
        echo -e "\nðŸ—ƒï¸ Weekly databases:"
        ls -lah charts_archive/databases/*.db 2>/dev/null || echo "No databases found"
        
        echo -e "\nðŸ’¾ Backups:"
        ls -lah charts_archive/backups/*.db 2>/dev/null || echo "No backups found"
        
        echo -e "\nðŸ“Š Latest CSV:"
        ls -lah charts_archive/latest_chart.csv 2>/dev/null || echo "No CSV file"
        
        echo -e "\nðŸ“ˆ File statistics:"
        find charts_archive -type f -exec ls -lh {} \; | awk '{print $5, $9}'
    
    # 7. Commit and push changes
    - name: ðŸ“¤ Commit and push changes
      run: |
        echo "ðŸ“ Preparing commit..."
        
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        
        # Add only archive files
        git add charts_archive/
        
        if git diff --cached --quiet; then
          echo "ðŸ“­ No changes to commit"
        else
          DATE=$(date +'%Y-%m-%d')
          WEEK=$(date +'%Y-W%W')
          git commit -m "ðŸ“Š YouTube Chart Update ${DATE} (Week ${WEEK}) [Automated]"
          
          echo "â¬†ï¸ Pushing changes..."
          git push origin HEAD:main
          echo "âœ… Changes pushed to repository"
        fi
    
    # 8. Upload artifacts for debugging (only on failure)
    - name: ðŸ“¦ Upload artifacts (on failure)
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: chart-debug-${{ github.run_number }}
        path: |
          charts_archive/
        retention-days: 7
    
    # 9. Final report
    - name: ðŸ“‹ Final report
      if: always()
      run: |
        echo "========================================"
        echo "ðŸŽµ EXECUTION FINAL REPORT"
        echo "========================================"
        echo "ðŸ“… Date: $(date)"
        echo "ðŸ“Œ Trigger: ${{ github.event_name }}"
        echo "ðŸ”— Commit: ${{ github.sha }}"
        echo ""
        
        WEEK_ID=$(python3 -c "from datetime import datetime; y,w,_ = datetime.now().isocalendar(); print(f'{y}-W{w:02d}')")
        DB_FILE="charts_archive/databases/youtube_charts_${WEEK_ID}.db"
        
        if [ -f "$DB_FILE" ]; then
          echo "âœ… Current week database updated: $DB_FILE"
          SIZE=$(stat -f%z "$DB_FILE" 2>/dev/null || stat -c%s "$DB_FILE")
          echo "ðŸ“Š Size: $((SIZE / 1024)) KB"
        else
          echo "âš ï¸ Current week database not found"
        fi
        
        if [ -f "charts_archive/latest_chart.csv" ]; then
          echo "âœ… Latest CSV updated"
          LATEST_SIZE=$(stat -f%z "charts_archive/latest_chart.csv" 2>/dev/null || stat -c%s "charts_archive/latest_chart.csv")
          echo "ðŸ“Š Size: $((LATEST_SIZE / 1024)) KB"
        fi
        
        TOTAL_DBS=$(find charts_archive/databases -name "youtube_charts_*.db" 2>/dev/null | wc -l)
        if [ "$TOTAL_DBS" -gt 0 ]; then
          echo ""
          echo "ðŸ“¦ Total historical databases: $TOTAL_DBS"
        fi
        
        TOTAL_BACKUPS=$(find charts_archive/backups -name "backup_*.db" 2>/dev/null | wc -l)
        if [ "$TOTAL_BACKUPS" -gt 0 ]; then
          echo "ðŸ’¾ Total backups available: $TOTAL_BACKUPS"
        fi
        
        echo ""
        echo "âœ… Process completed"
        echo "========================================"
