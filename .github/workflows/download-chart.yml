name: Download YouTube Chart

on:
  schedule:
    # Run every Monday at 12:00 UTC
    - cron: '0 12 * * 1'
  
  # Allow manual workflow execution
  workflow_dispatch:
  
  # Trigger on push to main branch if Python scripts change
  push:
    branches:
      - main
    paths:
      - 'scripts/*.py'

env:
  # Number of days to retain artifacts
  RETENTION_DAYS: 30

jobs:
  download-and-store:
    name: Download and Store YouTube Charts
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    # Required permissions for committing changes
    permissions:
      contents: write
    
    steps:
    # Step 1: Checkout repository
    - name: ðŸ“š Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for proper git operations
    
    # Step 2: Set up Python environment
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
        cache: 'pip'  # Cache pip dependencies for faster builds
    
    # Step 3: Install Python dependencies
    - name: ðŸ“¦ Install dependencies
      run: |
        pip install -r requirements.txt
        python -m playwright install chromium
        python -m playwright install-deps
    
    # Step 4: Create required directory structure
    - name: ðŸ“ Create directory structure
      run: |
        mkdir -p data charts_archive/1_download-chart/databases charts_archive/1_download-chart/backup
    
    # Step 5: Execute download script
    - name: ðŸš€ Run download script
      run: |
        python scripts/1_download.py
      env:
        GITHUB_ACTIONS: true
    
    # Step 6: Verify execution results
    - name: âœ… Verify results
      run: |
        echo "ðŸ“Š Verifying execution results..."
        
        echo "ðŸ“‚ Contents of charts_archive/1_download-chart/:"
        ls -lah charts_archive/1_download-chart/
        
        echo -e "\nðŸ—ƒï¸ Weekly databases:"
        ls -lah charts_archive/1_download-chart/databases/*.db 2>/dev/null || echo "No databases found"
        
        echo -e "\nðŸ’¾ Backups:"
        ls -lah charts_archive/1_download-chart/backup/*.db 2>/dev/null || echo "No backups found"
        
        echo -e "\nðŸ“Š Latest CSV:"
        ls -lah charts_archive/1_download-chart/latest_chart.csv 2>/dev/null || echo "No CSV found"
        
        echo -e "\nðŸ“ˆ File statistics:"
        find charts_archive/1_download-chart -type f -exec ls -lh {} \; | awk '{print $5, $9}'
    
    # Step 7: Commit and push changes to repository
    - name: ðŸ“¤ Commit and push changes
      run: |
        echo "ðŸ“ Preparing commit..."
        
        # Configure git user for automated commits
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        
        # Stage only archive files
        git add charts_archive/
        
        # Check if there are changes to commit
        if git diff --cached --quiet; then
          echo "ðŸ”­ No changes to commit"
        else
          DATE=$(date +'%Y-%m-%d')
          WEEK=$(date +'%Y-W%W')
          git commit -m "ðŸ“Š YouTube Chart Update ${DATE} (Week ${WEEK}) [Automated]"
          
          echo "â¬†ï¸ Pushing changes to repository..."
          git push origin HEAD:main
          echo "âœ… Changes pushed successfully"
        fi
    
    # Step 8: Upload artifacts for debugging (only on failure)
    - name: ðŸ“¦ Upload artifacts (on failure)
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: chart-debug-${{ github.run_number }}
        path: |
          data/
          charts_archive/
        retention-days: 7
    
    # Step 9: Generate final execution report
    - name: ðŸ“‹ Final report
      if: always()
      run: |
        echo "========================================"
        echo "ðŸŽµ FINAL EXECUTION REPORT"
        echo "========================================"
        echo "ðŸ“… Date: $(date)"
        echo "ðŸ“Œ Trigger: ${{ github.event_name }}"
        echo "ðŸ”— Commit: ${{ github.sha }}"
        echo ""
        
        # Calculate current week identifier
        WEEK_ID=$(python3 -c "from datetime import datetime; y,w,_ = datetime.now().isocalendar(); print(f'{y}-W{w:02d}')")
        DB_FILE="charts_archive/1_download-chart/databases/youtube_charts_${WEEK_ID}.db"
        
        # Check if weekly database was created/updated
        if [ -f "$DB_FILE" ]; then
          echo "âœ… Weekly database updated: $DB_FILE"
          SIZE=$(stat -c%s "$DB_FILE")
          echo "ðŸ“Š Size: $((SIZE / 1024)) KB"
        else
          echo "âš ï¸ Weekly database not found"
        fi
        
        # Check if latest CSV was created
        if [ -f "charts_archive/1_download-chart/latest_chart.csv" ]; then
          echo "âœ… Latest CSV updated"
          LATEST_SIZE=$(stat -c%s "charts_archive/1_download-chart/latest_chart.csv")
          echo "ðŸ“Š Size: $((LATEST_SIZE / 1024)) KB"
        fi
        
        # Count total historical databases
        TOTAL_DBS=$(find charts_archive/1_download-chart/databases -name "youtube_charts_*.db" 2>/dev/null | wc -l)
        if [ "$TOTAL_DBS" -gt 0 ]; then
          echo ""
          echo "ðŸ“¦ Total historical databases: $TOTAL_DBS"
        fi
        
        # Count available backups
        TOTAL_BACKUPS=$(find charts_archive/1_download-chart/backup -name "backup_*.db" 2>/dev/null | wc -l)
        if [ "$TOTAL_BACKUPS" -gt 0 ]; then
          echo "ðŸ’¾ Total available backups: $TOTAL_BACKUPS"
        fi
        
        echo ""
        echo "âœ… Process completed"
        echo "========================================"
